# -*- coding: utf-8 -*-
"""SNS Project .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oUAWftASPTLQiIWE-bM2tFPrf3s0r1JB
"""

!pip install -U scipy matplotlib

from scipy.io.wavfile import write

import numpy as np
from matplotlib import pyplot as plt

SAMPLE_RATE = 44100  # Hertz
DURATION = 50  # Seconds

from scipy.fft import fft, fftfreq

# for data transformation 
import numpy as np 
# for visualizing the data 
import matplotlib.pyplot as plt 
# for opening the media file 
import scipy.io.wavfile as wavfile

Fs, aud = wavfile.read('/content/Audio Semester Project.wav') 
aud = aud[:,0] # select left channel only
first = aud[:int(Fs*43)] # trim the first 43 seconds

powerSpectrum, frequenciesFound, time, imageAxis = plt.specgram(first, Fs=Fs) 
plt.show()

import subprocess

!sudo apt-get install libportaudio2
!pip install sounddevice

import sounddevice

# import Audio and Spectrogram classes from OpenSoundscape
from opensoundscape.audio import Audio
from opensoundscape.spectrogram import Spectrogram

!pip install OpenSoundscape

audio_filename = '/content/drive/MyDrive/SNS project/Audio Semester Project.wav'

audio_object = Audio.from_file(audio_filename)
spectrogram_object = Spectrogram.from_audio(audio_object)

from pathlib import Path

# Settings
image_shape = (224, 224) #(height, width) not (width, height)
image_save_path = Path('/content/drive/MyDrive/SNS project/image.png')

# Load audio file as Audio object
audio = Audio.from_file(audio_filename)

# Create Spectrogram object from Audio object
spectrogram = Spectrogram.from_audio(audio)

# Convert Spectrogram object to Python Imaging Library (PIL) Image
image = spectrogram.to_image(shape=image_shape,invert=True)

# Save image to file
image.save(image_save_path)

Spectrogram.from_audio(Audio.from_file(audio_filename)).to_image(shape=image_shape,invert=True).save(image_save_path)

print(f"How many samples does this audio object have? {len(audio_object.samples)}")
print(f"What is the sampling rate? {audio_object.sample_rate}")

audio_object = Audio.from_file(audio_filename)

length = audio_object.duration()
print(length)

trimmed = audio_object.trim(0,40)
trimmed.duration()

# Commented out IPython magic to ensure Python compatibility.
# calculate the fft
fft_spectrum, frequencies = trimmed.spectrum()

#plot settings
from matplotlib import pyplot as plt
plt.rcParams['figure.figsize']=[15,5] #for big visuals
# %config InlineBackend.figure_format = 'retina'

# plot
plt.plot(frequencies,fft_spectrum)
plt.ylabel('Fast Fourier Transform (V**2/Hz)')
plt.xlabel('Frequency (Hz)')

audio_object = Audio.from_file(audio_filename)
spectrogram_object = Spectrogram.from_audio(audio_object)

spec = Spectrogram.from_audio(Audio.from_file(audio_filename))
print(f'All times: {spec.times[0:40]}')
print(f'All frequencies frequencies: {spec.frequencies[0:40]}')

audio_object = Audio.from_file(audio_filename)
spectrogram_object = Spectrogram.from_audio(audio_object)
spectrogram_object.plot()

spec_trimmed = spec.trim(0, 50)
spec_trimmed.plot()

# calculate amplitude signal
high_freq_amplitude = spec_trimmed.amplitude()

# plot
from matplotlib import pyplot as plt
plt.plot(spec_trimmed.times,high_freq_amplitude)
plt.xlabel('time (sec)')
plt.ylabel('amplitude')
plt.show()

baww_low_freq = 2500
baww_high_freq = 6500

spec_bandpassed = spec_trimmed.bandpass(baww_low_freq, baww_high_freq)
spec_bandpassed.plot()

from opensoundscape.torch.models.cnn import load_model
import opensoundscape

# Other utilities and packages
import torch
from pathlib import Path
import numpy as np
import pandas as pd
from glob import glob
import subprocess

# Commented out IPython magic to ensure Python compatibility.
#set up plotting
from matplotlib import pyplot as plt
plt.rcParams['figure.figsize']=[40,2] #for large visuals
# %config InlineBackend.figure_format = 'retina'

from opensoundscape.torch.models.cnn import CNN
CNN('resnet18',['classA','classB','classC','classD','ClassE'],8.0).save('./temp.model')

model = load_model('./temp.model')

audio_files=("/content/Audio Semester Project.wav")

from glob import glob
audio_files = glob('./*.wav') #match all .wav files in the current directory
audio_files

scores, _, _ = model.predict(audio_files, overlap_fraction=0.5)
scores.head()

scores, _, _ = model.predict(audio_files)
scores.head()

from opensoundscape.preprocess.utils import show_tensor_grid
from opensoundscape.torch.datasets import AudioSplittingDataset

#generate a dataset with the samples we wish to generate and the model's preprocessor
inspection_dataset = AudioSplittingDataset(audio_files, model.preprocessor)
inspection_dataset.bypass_augmentations = True

samples = [sample['X'] for sample in inspection_dataset]
_ = show_tensor_grid(samples,4)

scores, binary_predictions, _ = model.predict(
    audio_files,
    activation_layer='softmax',
    binary_preds='single_target'
)

scores.head()

binary_predictions.head()

